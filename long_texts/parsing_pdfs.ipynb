{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "from langchain_community.document_loaders import PDFMinerPDFasHTMLLoader\n",
    "from typing import Literal\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from pdfminer.high_level import extract_text_to_fp\n",
    "from pdfminer.layout import LAParams\n",
    "\n",
    "output_string = StringIO()\n",
    "with open(\"2308.04014v2.pdf\", \"rb\") as fin:\n",
    "    extract_text_to_fp(\n",
    "        fin, output_string, laparams=LAParams(), output_type=\"html\", codec=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = output_string.getvalue().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301245"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r\"C:\\Users\\tomas\\Downloads\\2308.04014v2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PDFMinerPDFasHTMLLoader(PATH)\n",
    "data = loader.load()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(text, \"html.parser\")\n",
    "content = soup.find_all(\"div\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "cur_fs = None\n",
    "cur_text = \"\"\n",
    "snippets = []  # first collect all snippets that have the same font size\n",
    "for c in content:\n",
    "    sp = c.find(\"span\")\n",
    "    if not sp:\n",
    "        continue\n",
    "    st = sp.get(\"style\")\n",
    "    if not st:\n",
    "        continue\n",
    "    fs = re.findall(\"font-size:(\\d+)px\", st)\n",
    "    if not fs:\n",
    "        continue\n",
    "    fs = int(fs[0])\n",
    "    if not cur_fs:\n",
    "        cur_fs = fs\n",
    "    if fs == cur_fs:\n",
    "        cur_text += c.text\n",
    "    else:\n",
    "        snippets.append((cur_text, cur_fs))\n",
    "        cur_fs = fs\n",
    "        cur_text = c.text\n",
    "snippets.append((cur_text, cur_fs))\n",
    "# Note: The above logic is very straightforward. One can also add more strategies such as removing duplicate snippets (as\n",
    "# headers/footers in a PDF appear on multiple pages so if we find duplicates it's safe to assume that it is redundant info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.docstore.document import Document\n",
    "\n",
    "cur_idx = -1\n",
    "semantic_snippets = []\n",
    "# Assumption: headings have higher font size than their respective content\n",
    "for s in snippets:\n",
    "    # if current snippet's font size > previous section's heading => it is a new heading\n",
    "    if (\n",
    "        not semantic_snippets\n",
    "        or s[1] > semantic_snippets[cur_idx].metadata[\"heading_font\"]\n",
    "    ):\n",
    "        metadata = {\"heading\": s[0], \"content_font\": 0, \"heading_font\": s[1]}\n",
    "        metadata.update(data.metadata)\n",
    "        semantic_snippets.append(Document(page_content=\"\", metadata=metadata))\n",
    "        cur_idx += 1\n",
    "        continue\n",
    "\n",
    "    # if current snippet's font size <= previous section's content => content belongs to the same section (one can also create\n",
    "    # a tree like structure for sub sections if needed but that may require some more thinking and may be data specific)\n",
    "    if (\n",
    "        not semantic_snippets[cur_idx].metadata[\"content_font\"]\n",
    "        or s[1] <= semantic_snippets[cur_idx].metadata[\"content_font\"]\n",
    "    ):\n",
    "        semantic_snippets[cur_idx].page_content += s[0]\n",
    "        semantic_snippets[cur_idx].metadata[\"content_font\"] = max(\n",
    "            s[1], semantic_snippets[cur_idx].metadata[\"content_font\"]\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    # if current snippet's font size > previous section's content but less than previous section's heading than also make a new\n",
    "    # section (e.g. title of a PDF will have the highest font size but we don't want it to subsume all sections)\n",
    "    metadata = {\"heading\": s[0], \"content_font\": 0, \"heading_font\": s[1]}\n",
    "    # metadata.update(data.metadata)\n",
    "    semantic_snippets.append(Document(page_content=\"\", metadata=metadata))\n",
    "    cur_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Kshitij Gupta * 1 2 Benjamin Th´erien * 1 2 Adam Ibrahim * 1 2 Mats L. Richter 1 2 Quentin Anthony 1 2 3\\nEugene Belilovsky 4 1 2 Irina Rish 1 2 Timoth´ee Lesort 1 2\\n', metadata={'heading': 'Continual Pre-Training of Large Language Models: How to (re)warm your\\nmodel?\\n', 'content_font': 9, 'heading_font': 14, 'source': 'C:\\\\Users\\\\tomas\\\\Downloads\\\\2308.04014v2.pdf'}),\n",
       " Document(page_content='3\\n2\\n0\\n2\\np\\ne\\nS\\n6\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n2\\nv\\n4\\n1\\n0\\n4\\n0\\n.\\n8\\n0\\n3\\n2\\n:\\nv\\ni\\nX\\nr\\na\\nLarge language models (LLMs) are routinely pre-\\ntrained on billions of tokens, only to restart the\\nprocess over again once new data becomes avail-\\nable. A much cheaper and more efficient solution\\nwould be to enable the continual pre-training of\\nthese models, i.e. updating pre-trained models\\nwith new data instead of re-training them from\\nscratch. However, the distribution shift induced\\nby novel data typically results in degraded per-\\nformance on past data. Taking a step towards\\nefficient continual pre-training, in this work, we\\nexamine the effect of different warm-up strate-\\ngies. Our hypothesis is that the learning rate must\\nbe re-increased to improve compute efficiency\\nwhen training on a new dataset. We study the\\nwarmup phase of models pre-trained on the Pile\\n(upstream data, 300B tokens) as we continue to\\npre-train on SlimPajama (downstream data, 297B\\ntokens), following a linear warmup and cosine\\ndecay schedule. We conduct all experiments on\\nthe Pythia 410M language model architecture and\\nevaluate performance through validation perplex-\\nity. We experiment with different pre-training\\ncheckpoints, various maximum learning rates, and\\nvarious warmup lengths. Our results show that\\nwhile rewarming models first increases the loss\\non upstream and downstream data, in the longer\\nrun it improves the downstream performance, out-\\nperforming models trained from scratch—even\\nfor a large downstream dataset.\\n*Equal contribution; authorship order determined by a coinflip\\n1Department of Computer Science and Operation Research, Uni-\\nversit´e de Montr´eal, Montr´eal, Canada 2Mila, Montr´eal, Canada\\n3Eleuther AI 4Department of Computer Science and Software Engi-\\nneering, Concordia University, Montr´eal, Canada. Correspondence\\nto: Benjamin Th´erien <benjamin.therien@umontreal.ca>.\\nWork presented at the ES-FoMo Workshop at the 40 th Interna-\\ntional Conference on Machine Learning, Honolulu, Hawaii, USA.\\nPMLR 202, 2023. Copyright 2023 by the author(s).\\n1\\nLarge pre-trained models have enabled massive performance\\nimprovements for many downstream tasks in vision (Kir-\\nillov et al., 2023; Oquab et al., 2023) and language (Brown\\net al., 2020; Zhao et al., 2023). However, training these foun-\\ndation models is prohibitively expensive. Existing works\\naim to reduce the cost of large-scale model development\\nby enabling low-cost hyperparameter optimization (Yang\\net al., 2022) or providing guidelines for maximizing per-\\nformance under a given compute budget (Hoffmann et al.,\\n2022). However, these works assume that models will be\\ntrained from scratch. As the amount of data available for pre-\\ntraining is ever-growing, new and improved datasets (e.g.\\nRedPajama and SlimPajama (Together.xyz, 2023; Soboleva\\net al., 2023; Touvron et al., 2023)) will continue to become\\navailable. Should practitioners always combine existing\\ndatasets (e.g. Pile (Gao et al., 2020)) and train from scratch\\nto obtain the best performance? Doing so would quickly be-\\ncome prohibitively expensive and fails to leverage existing\\npre-trained models.\\nOur approach circumvents the need for complete re-training\\nby continuing to pre-train existing models on new data.\\nWe refer to this as “continual pre-training” and the goal is\\nto minimize the loss on new data while maintaining low\\nloss on previous data. Continual pre-training is a critical\\nchallenge since it can lead to catastrophic forgetting (French,\\n1999). Moreover, the potential long sequence of training\\nstages may make common continual learning techniques\\nsuch as replay (Rebuffi et al., 2017; Ostapenko et al., 2022)\\nor regularisation (Kirkpatrick et al., 2017; Farajtabar et al.,\\n2020) not compute efficient enough (Lesort et al., 2023). A\\nsimple and – from a compute cost perspective – scalable\\nsolution to limit forgetting in such situations is to (only)\\nprogressively decrease the learning rate every time new data\\nbecomes available (Mirzadeh et al., 2020; Winata et al.,\\n2023). However, this solution is limited because repeatedly\\ndecreasing the learning rate would cause it to eventually\\nbecome too small if the number of training stages becomes\\nhigh.\\nIn this work, we take a step towards efficient continual pre-\\ntraining by studying how to re-increase a small learning\\nContinual Pre-Training of Large Language Models: How to (re)warm-up your model?\\nrate to keep training a pre-trained language model on new\\ndata. We refer to this as re-warming the model. Re-warming\\nthe model should improve learning efficiency by avoiding\\na vanishing learning rate. We study warm-up strategies on\\nPythia 410M model with various amounts of data, maximum\\nlearning rates and different pre-trained checkpoints. This\\nwould allow a model trained initially on a large dataset to\\nbenefit from resuming training on a newer large dataset\\nwithout having to retrain from scratch. In order to simulate\\nthis setting, we fix our initial pre-training dataset to be Pile\\nand the newer dataset to be SlimPajama. We hope that this\\nmay guide the adaptation of existing LLMs to future new\\ndatasets.\\nOur results show that:\\n1. Progressively increasing the learning rate to warm-up\\nis not necessary but starting directly from the maxi-\\nmum learning rate creates an initial large spike in the\\nloss (chaotic phase a.k.a stability gap) with no conse-\\nquences later.\\n2. Adjusting the maximum learning rate can help trade-\\noff between upstream and downstream performance;\\nincreasing the maximum learning rate leads to stronger\\nadaptation to the downstream dataset (SlimPajama),\\nwhile smaller learning rates preserve more perfor-\\nmance on the upstream dataset (Pile).\\n3. Continual pre-training with the latest pre-trained check-\\npoint improves performance.\\n', metadata={'heading': 'Abstract\\n1. Introduction\\n', 'content_font': 10, 'heading_font': 11}),\n",
       " Document(page_content='In our setup, the upstream (or pre-training) dataset is the Pile\\n(Gao et al., 2020). The downstream (or fine-tuning) dataset\\nis SlimPajama (Soboleva et al., 2023). SlimPajama is an ex-\\ntensively deduplicated version of RedPajama (Together.xyz,\\n2023) which is built based on the LLama dataset (Touvron\\net al., 2023). In this work, we use “fine-tuning” and down-\\nstream continual pre-training interchangeably. However, in\\nour continual pre-training setting, we note that the down-\\nstream dataset is on the scale of the previous pre-training\\ndataset (i.e. very large, unlike many fine-tuning datasets).\\nThe SlimPajama dataset is built from similar sources as the\\nPile but with a higher quantity of data. Therefore, some\\nupstream data may be repeated during downstream pre-\\ntraining. Our experimental setup is comparable to the setup\\nof (Ash & Adams, 2020), where they train a classifier on\\nhalf of the samples of a dataset first, and fine-tune it later\\non all samples. They show that warm starting for image\\nclassification is challenging. Using a model pre-trained on\\nthe Pile and continuing the pre-training on SlimPajama, we\\nfollow an analogous setup for causal language modeling.\\nDatasets – We use the Pile with the same weights as Black\\net al. (2022) for validation. We shuffle and randomly sample\\nTable 1. Token counts and train data weights for our subsampled\\nversion of SlimPajama.\\nDataset\\nSampling %\\nTrain\\nVal\\nStackExchange\\nArxiv\\nWikipedia\\nBook\\nGithub\\nC4\\nCommoncrawl\\nTotals\\n2.0\\n2.5\\n4.5\\n4.5\\n4.5\\n15.0\\n67.0\\n9.95B\\n13.08M\\n13.77B\\n22.73M\\n11.78B\\n15.79M\\n14.22B\\n22.04M\\n15.41B\\n22.42M\\n72.49M\\n78.49B\\n153.25B 147.28M\\n100\\n296.86B 315.83M\\nthe SlimPajama dataset (Soboleva et al., 2023) to form the\\n∼297B token training dataset and ∼316M validation token\\ndataset. We do not use replay. We use the same tokenizer as\\n(Black et al., 2022) that is trained specifically on the Pile.\\nModel – We use the 410M Pythia pre-trained on the Pile\\n(Biderman et al., 2023), i.e. GPT-NeoX (Black et al., 2022)\\nmodels. We do not use flash attention (Dao et al., 2022).\\nHyperparameters – We use the AdamW optimizer with\\nβ1 = 0.9, β2 = 0.95, ϵ = 10−8, and a weight decay of 0.1.\\nThe maximum learning rate is varied in our experiments\\n{1.5 · 10−4, 3 · 10−4, 6 · 10−4}. We use cosine learning rate\\ndecay to a minimum of 0.1 · MaxLr. All warmup lengths\\nare calculated based on the full downstream dataset size\\n(297B tokens). We note that our cosine decay schedule\\nreaches the minimum learning rate at 240B tokens and is\\nconstant thereafter. We set gradient clipping to 1.0. Training\\nis conducted at half-precision (FP16), without dropout.\\n', metadata={'heading': '2. Setup\\n', 'content_font': 9, 'heading_font': 11}),\n",
       " Document(page_content='Large Language Models: LLMs are usually trained with\\nAdam (e.g., GPT3 (Brown et al., 2020), BLOOM (Scao\\net al., 2022), Gopher (Rae et al., 2021), Pythia (Biderman\\net al., 2023)) or AdamW (e.g., Chinchilla (Hoffmann et al.,\\n2022), LLaMA (Touvron et al., 2023)). In all the afore-\\nmentioned models, the learning rate schedule consists of a\\nwarm-up followed by a cosine decay to 10% of the maxi-\\nmum learning rate.\\nUnsupervised Continual Learning: In this paper, we in-\\nvestigate various warm-up strategies for the continual pre-\\ntraining of LLMs. Continual pre-training uses a similar\\ntype of training objectives as continual self-supervised train-\\ning. Self-supervised pre-training was also studied in vision\\ndatasets for image generation (Seff et al., 2017; Lesort et al.,\\n2019; Zhai et al., 2019; Nguyen et al., 2018; Davari et al.,\\n2022) or representation learning (Fini et al., 2022; Madaan\\net al., 2021; Rao et al., 2019). In language, continual pre-\\ntraining was studied under the name of domain adaptation\\n2\\nContinual Pre-Training of Large Language Models: How to (re)warm-up your model?\\npre-training (Ke et al., 2023a; Scialom et al., 2022; Guru-\\nrangan et al., 2021; Qin et al., 2022) where the new dataset\\ncomes from a new domain. Another setting is where differ-\\nent datasets are generated at different points in time (Han\\net al., 2021; Jin et al., 2022; Jang et al., 2021; 2022; Loureiro\\net al., 2022). In our setup, the scenario is closer to domain\\nadaptation pre-training, because we do not take into account\\nthe temporality of data.\\nMonitoring Learning Rate for Continual Training of\\nLanguage Models: In continual learning (CL), models are\\ntrained on sequences of datasets. Therefore, the data is\\nnot independent and identically distributed which can lead\\nthe model to lose plasticity or forget. In such situations,\\nparticular monitoring of the learning rate schedule can be\\nbeneficial. In CL of language models (Caccia et al., 2021;\\nKe et al., 2023a; Loureiro et al., 2022; Han et al., 2021;\\nLoshchilov & Hutter, 2018; Scialom et al., 2022; Winata\\net al., 2023) different approaches have been evaluated: con-\\nstant learning rate (Ke et al., 2023a; Scialom et al., 2022),\\nprogressive decrease (Winata et al., 2023) or warm-up then\\ndecrease (Caccia et al., 2021).\\nHowever, to the best of our knowledge, no existing work\\nstudies specifically the influence of the warm-up phase in the\\ncontext of continual pre-training for large language models.\\n', metadata={'heading': '3. Related Work\\n', 'content_font': 9, 'heading_font': 11}),\n",
       " Document(page_content='4.1. How long to warm up?\\nIn the literature, warm-up is usually conducted on at most\\n1% of the data (Zhao et al., 2023). In this experiment, we\\ninvestigate if the results are sensitive to this hyper-parameter.\\nSetup: We experiment with different warm-up lengths for\\na schedule of 297B tokens: 0%, 0.5%, 1%, and 2% of the\\ndata and measure the performance after the first 50B tokens.\\nFrom a different perspective, we could see this experiment\\nas running a 1% warm-up on different amounts of data. We\\nhypothesize that warming up for a larger number of itera-\\ntions could lead to a smoother transition with subsequent\\nperformance improvements.\\nResults: The results of this experiment are provided in\\nFig. 1. They show that the amount of data used for warming\\nup the learning rate does not significantly influence the per-\\nplexity on the downstream task (learning) or the upstream\\ntask (forgetting). These results invalidate our hypothesis\\nthat using more tokens for warm-up can smooth the transi-\\ntion and show that linear warmup is useless in this setting.\\nNevertheless, the model trained without any progressive\\nwarm up experiences an initial “choatic phase” causing a\\nspike in the loss in its first few iterations of training, this\\nphenomenon is also referred to as stability gap (Lange et al.,\\n2023; Caccia et al., 2022).\\nFigure 1. (top) Evolution of perplexity on SlimPajama while fine-\\ntuning with various amounts of tokens for warm-up. (bottom)\\nperplexity on the same experiments on the Pile validation set\\n(upstream). MaxLr = 3 · 10−4, MinLr = 0.1 · MaxLr. This\\nfigure shows that at that scale, the length of the warm-up phase\\ndoes not significantly influence results.\\nTakeaway 1:\\n• The length of the warmup phase does not ap-\\npear to have a significant effect on the Pile and\\nSlimPajama validation losses.\\n4.2. How high to warm up?\\nOne objective of re-warming the learning rate is to enable\\ncompute-efficient continual pre-training. A learning rate\\nthat is too small may lead to inefficient learning on the\\ndownstream dataset, whereas, a learning rate that is too\\nlarge may lead to catastrophic forgetting of the upstream\\ndataset. One important aspect of re-warming the learning\\nrate is to decide how high to increase it. Therefore, in this\\nexperiment, we vary the maximum learning rate to assess\\nits effect on performance.\\nSetup: We fix the length of the warm-up phase to the default\\namount of 1% of the training data and vary the maximum\\nlearning rate. We experiment with the default value of\\n3 · 10−4 used for pre-training Pythia 410M (Biderman et al.,\\n2023), 1.5 · 10−4, and 6 · 10−4. For the post-warmup cosine\\ndecay phase, we set the final learning rate to 10% of the\\n3\\nContinual Pre-Training of Large Language Models: How to (re)warm-up your model?\\nmaximum learning rate. The learning rate schedule we used\\ndecays to the minimum learning rate at 240B tokens and\\nis constant thereafter. The runs are reported to the end of\\n240B tokens (the end of decay period).\\nFigure 2. Evolution of loss on SlimPajama for different maximum\\nlearning rates. The blue curve reports a model trained from scratch.\\nGrowing the maximum learning rate consistently decreases the\\nfinal loss on downstream data. At convergence, the models being\\ncontinually pre-trained outperform the scratch and constant LR\\nbaselines. However, the constant learning rate model achieves best\\nperformance within the first 100B tokens.\\nFigure 3. Evolution of loss on Pile for different maximum learning\\nrates. The blue curve reports a model trained from scratch. Grow-\\ning the maximum learning rate consistently increases the final loss\\non upstream data, i.e. it increases forgetting. The from-scratch\\nbaseline consistently improves its performance on Pile, while being\\ntrained on SlimPajama, showing the significant synergy between\\nboth datasets.\\nResults: The results of this experiment are provided in fig-\\nures 2, 3, and 4. We observe, at the end of training, that\\nlarger maximum learning rates improve performance on\\ndownstream data, while they hurt performance on upstream\\ndata. Conversely, a smaller maximum learning rate im-\\nproves performance on upstream data, while limiting adap-\\ntation to downstream data—causing decreased performance.\\nThese findings show that altering the maximum learning rate\\ncan be an effective way to tradeoff between downstream\\nand upstream performance. Additionally, we observe a gen-\\nFigure 4. Perplexity downstream vs perplexity upstream, RP fine-\\ntuning. Green points refer to the ends of the warm-up phases. The\\nred point represents the perplexity before starting the downstream\\nfine-tuning. Increasing the maximum learning rate improves per-\\nformance on the downstream data, but causes forgetting on the\\nupstream. This plot reports the same results as figures 2 and 3.\\neral trend: fine-tuning on SlimPajama, causes the model\\nto forget what has been learned on the Pile leading to an\\nincrease in the Pile validation perplexity. Finally, we note\\nthat employing early stopping on the model trained from a\\nconstant learning rate (similar to traditional fine-tuning) is\\nan economical way of adapting to the new data distribution\\nwhile retaining strong performance on the upstream dataset.\\nTakeaway 2:\\n• Rewarming then decaying the learning rate\\nappears necessary to learn well on the down-\\nstream task. Moreover, while keeping a con-\\nstant learning is initially advantageous on Pile,\\nthis advantage vanishes when training long\\nenough on SlimPajama.\\n• A model that only learns on SlimPajama per-\\nforms worse on SlimPajama than models pre-\\ntrained on Pile in spite of being optimised\\nsolely for the downstream task, highlighting\\npositive transfer between the two datasets.\\n4.3. Comparing with from Scratch Training\\nIn this experiment, we want to compare finetuned models\\nwith models trained from scratch.\\nSetup: We train a model from random initialization using\\nthe same cosine decay schedule as the MaxLr = 3 · 10−4\\nmodel in Section 4.2.\\nResults: As we can see in Fig. 2 and Fig. 3, all the fine-\\ntuned models with a warm-up perform better than the model\\n4\\nContinual Pre-Training of Large Language Models: How to (re)warm-up your model?\\ntrained from scratch. This shows that finetuning instead of\\nretraining might improve performance even when the down-\\nstream dataset is on the scale of the upstream dataset and\\noverlaps with the upstream dataset. We also observe that,\\nafter 200B tokens, the model trained from scratch performs\\nbetter than the model finetuned using a constant learning\\nrate.\\n4.4. Re-warming on the same data\\nIn the previous experiments we have seen that finetuning on\\nnew data leads to a quick increase of loss on past data, that\\ndecrease later. The increase is higher when the max learning\\nrate is bigger. One hypothesis for the increase in loss is that\\nthe distribution shift between upstream and downstream data\\ndisturbs the training process. To assess this hypothesis, we\\napply our warm-up policy in a setting with no distribution\\nshift. That is, we replicate our experiments from figures 3\\nand 4 by fine-tuning on Pile.\\nFigure 5. Pile validation loss while fine-tuning again on the Pile.\\nWarm-up phenomenon observed in Sec. 4.2 is also observed ap-\\nplied to fine-tuning again on the same data distribution. Warm-up\\ntoken=1% downstream tokens, MinLr = 0.1 · MaxLr.\\nSetup: In this experiment, instead of fine-tuning on SlimPa-\\njama data, we fine-tune on 50B tokens of the Pile data with\\nthe same parametrization of the warm-up policy as Sec. 4.2\\nexperiments.\\nResults: Fig. 5, shows that re-warming the learning rate\\nwhile continuing to pre-train on the Pile has a similar effect\\nas re-warming on SlimPajama data Fig. 3 when looking\\nat the downstream validation loss. This suggests that the\\ndistribution shift between Pile and SlimPajama is not solely\\nto blame for the negative impact of re-warming the learning\\nrate observed in sec. 4.2, and that the optimization dynamics\\nalso plays a role in this increase of loss.\\nFig. 6 shows that the training first increases perplexity on\\nboth the Pile and SlimPajama data but reduces after on\\nboth. Interestingly, Fig. 6 show a linear relationship between\\nSlimPajama perplexity and the Pile perplexity when fine-\\ntuning on the Pile, while it was not the case while fine-\\nFigure 6. Perplexity on the Pile vs perplexity on SlimPajama when\\nfine-tuning on the Pile with various maximum learning rates.\\nWarm-up token=1% downstream tokens, MinLr = 0.1 · MaxLr.\\nGreen points refer to the end of the warm-up phase.\\ntuning on SlimPajama (Fig. 3). One possible explanation\\nfor this relationship is that models trained on Pile climb out\\nof a minimum during warmup and return towards the same\\nminimum as the learning rate is decayed, yielding the linear\\ntrend.\\nTakeaway 3:\\n• Rewarming the learning rate appears to be a\\nsignificant cause for the degradation of perfor-\\nmance seen previously when starting to learn\\non the downstream task, as evidenced by re-\\nwarming then decaying the learning rate while\\ntraining on the same dataset.\\n• The models do not appear to be able to recover\\nfrom the performance hit due to rewarming\\nthe learning rate when training on the same\\ndataset.\\n4.5. Evaluating Earlier Checkpoints\\nSetup: We select three checkpoints from model pre-training\\nto test if warm-up strategies benefit from starting with non-\\nconverged checkpoints. Our hypothesis is that selecting\\ncheckpoints farther from convergence may benefit adapta-\\ntion to the downstream task as these checkpoints may be\\nlocated at more favorable points in the loss landscape.\\nTo select significantly different checkpoints, we compare the\\nlast pre-training checkpoint (i.e. Pythia 410M after 143, 000\\niters), to an earlier checkpoint achieving a Pile validation\\nloss near the maximum Pile validation loss attained by all\\nmodels in Fig. 1 (bottom) (∼ 2.5), and a third checkpoint in\\nbetween the two other checkpoints.\\n5\\nContinual Pre-Training of Large Language Models: How to (re)warm-up your model?\\nadaptation pre-training setups (Xu et al., 2019; Gururangan\\net al., 2020; Ke et al., 2023a; Chakrabarty et al., 2019; Ke\\net al., 2023b). Nevertheless, comparing Fig. 4 and Fig. 6,\\nwe see that the results are not identical when fine-tuning\\non the Pile or when fine-tuning on SlimPajama. A possible\\nexplanation is that even a slight shift in data distribution can\\nlead to a significant perturbation of the learning dynamics.\\nFor example, in the context of image classification, Igl et al.\\n(2020) show how a sudden transition of 10 to 20 % of the\\nlabels in the dataset can have a significant impact on the\\ndownstream performance (see Fig. 5 of (Igl et al., 2020)).\\nExperiments Scale:\\nAs described in Sec. 2, our investigation explores models\\nof size 410M and fine-tuning dataset of size 297B tokens.\\nWhile this is a preliminary study, in future work, we plan\\nto verify whether our conclusions hold at different model\\nscales (e.g., 3B and 7B) and different dataset scales (e.g.,\\n100B and 600B). Moreover, we plan to test our models\\nthroughout using benchmarks such as HELM (Liang et al.,\\n2022) or Harness (Gao et al., 2021) instead of only loss\\nor perplexity, as these benchmarks can provide important\\ninsight into the evolution of model capabilities.\\n', metadata={'heading': '4. Continual Warm-up\\n', 'content_font': 9, 'heading_font': 11}),\n",
       " Document(page_content='Our experiments demonstrate that warming up to higher\\nmaximum learning rates helps models pre-trained on the\\nPile adapt to SlimPajama, while a smaller maximum learn-\\ning rater preserves performance on the pile. In both cases,\\nhowever, models that are rewarmed improve over models\\ntrained from scratch. These results motivate the use of con-\\ntinual pre-training on new datasets rather than restarting\\ntraining from scratch. More research is needed, however,\\nto establish similar results for larger model scales, differ-\\nent distribution shifts, and verify that this strategy can be\\napplied repeatedly to update models.\\nFigure 7. Pile validation loss of models trained from the fully con-\\nverged checkpoint, the upstream saturation point, and 1/2 of the\\nupstream saturation point. Black colour designs for the earlier\\ncheckpoint, red colour the latest checkpoint and blue colour the\\nin-between one.\\nResults: The evolution of the validation losses on SlimPa-\\njama are provided in Fig. 7 and the evolution of the vali-\\ndation losses on the Pile is provided in appendix A. We\\nsee in Fig. 7 that, in our setup, selecting earlier check-\\npoints for later fine-tuning does not lead to improvement\\nin downstream performance. Therefore, selecting the latest\\ncheckpoint is the best option. We can conclude that the\\npre-training did not lead the model into a loss of plasticity\\nthat would make the model difficult to re-warm.\\nLocal conclusion: The experiments conducted in this sec-\\ntion led to the conclusion that re-warming the pre-trained\\nmodel on new data is a challenging task, even when the\\ndownstream data is of similar provenance to the upstream\\ndata. Our results show that the amount of tokens used for\\nwarm-up does not significantly alter performance, grow-\\ning the maximum learning rate improves downstream per-\\nformance of the final model while decreasing it improves\\nupstream performance, and selecting earlier checkpoints\\ndecreases performance on both upstream and downstream\\ndata.\\nTakeaway 4:\\n', metadata={'heading': '6. Conclusion\\n', 'content_font': 9, 'heading_font': 11}),\n",
       " Document(page_content='• Using an earlier checkpoint when pretraining\\non the Pile does not lead to learning faster on\\nSlimPajama.\\nGPT-NeoX (Andonian et al., 2021), DeepSpeed (Rasley\\net al., 2020), nccl (NVIDIA, 2016), Apex (NVIDIA, 2019),\\nPytorch (Paszke et al., 2017), HuggingFace Transformers\\nlibrary (Wolf et al., 2020).\\n', metadata={'heading': 'Software and Data\\n', 'content_font': 9, 'heading_font': 11}),\n",
       " Document(page_content='Data similarity and overlapping:\\nIn our experimental\\nsetup, upstream and downstream data have a high similarity,\\nnotably because of data overlap. Since in continual learning,\\ndifferent types of shifts can lead to variations in performance\\n(Lesort et al., 2021), our results may not generalize to setups\\nwith different distribution shifts, such as language domain\\nWe acknowledge the support from Canada CIFAR AI Chair\\nProgram and from the Canada Excellence Research Chairs\\nProgram. We would also like to acknowledge funding from\\nthe FRQNT Doctoral (B2X) scholarship [B.T.], the scholar-\\nship for Artificial Intelligence of Universit´e de Montr´eal’s\\n6\\nContinual Pre-Training of Large Language Models: How to (re)warm-up your model?\\n´Etudes Sup´erieures et Postdoctorales, and a fellowship of\\nthe IFI program of the German Academic Exchange Service\\n(DAAD).This research was made possible thanks to the com-\\nputing resources on the Summit supercomputer, provided as\\na part of the INCITE program award “Scalable Foundation\\nModels for Transferable Generalist AI”. These resources\\nwere provided by the Oak Ridge Leadership Computing\\nFacility at the Oak Ridge National Laboratory, which is\\nsupported by the Office of Science of the U.S. Department\\nof Energy under Contract No. DE-AC05-00OR22725.\\n', metadata={'heading': '5. Discussion / Limitation\\nAcknowledgements\\n', 'content_font': 9, 'heading_font': 11}),\n",
       " Document(page_content='Andonian, A., Anthony, Q., Biderman, S., Black, S.,\\nGali, P., Gao, L., Hallahan, E., Levy-Kramer, J., Leahy,\\nC., Nestler, L., Parker, K., Pieler, M., Purohit, S.,\\nSongz, T., Phil, W., and Weinbach, S. GPT-NeoX:\\nLarge Scale Autoregressive Language Modeling in Py-\\nTorch, 8 2021. URL https://www.github.com/\\neleutherai/gpt-neox.\\nAsh, J. and Adams, R. P. On warm-starting neural network\\ntraining. Advances in neural information processing sys-\\ntems, 33:3884–3894, 2020.\\nBiderman, S., Schoelkopf, H., Anthony, Q., Bradley, H.,\\nO’Brien, K., Hallahan, E., Khan, M. A., Purohit, S.,\\nPrashanth, U. S., Raff, E., et al. Pythia: A suite for ana-\\nlyzing large language models across training and scaling.\\narXiv preprint arXiv:2304.01373, 2023.\\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\\nTow, J., Wang, B., and Weinbach, S. Gpt-neox-20b: An\\nopen-source autoregressive language model, 2022.\\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry,\\nG., Askell, A., et al. Language models are few-shot\\nlearners. In Proceedings of the 34th International Con-\\nference on Neural Information Processing Systems, pp.\\n1877–1901, 2020. URL https://arxiv.org/abs/\\n2005.14165.\\nCaccia, L., Xu, J., Ott, M., Ranzato, M., and Denoyer,\\nL. On anytime learning at macroscale. arXiv preprint\\narXiv:2106.09563, 2021.\\nCaccia, L., Aljundi, R., Asadi, N., Tuytelaars, T., Pineau,\\nJ., and Belilovsky, E. New insights on reducing\\nabrupt representation change in online continual learn-\\nIn International Conference on Learning Repre-\\ning.\\nsentations, 2022. URL https://openreview.net/\\nforum?id=N8MaByOzUfb.\\nChakrabarty, T., Hidey, C., and McKeown, K. IMHO fine-\\ntuning improves claim detection. In Proceedings of the\\n2019 Conference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human Lan-\\nguage Technologies, Volume 1 (Long and Short Papers),\\npp. 558–563, Minneapolis, Minnesota, June 2019. Asso-\\nciation for Computational Linguistics. doi: 10.18653/\\nv1/N19-1054. URL https://aclanthology.org/\\nN19-1054.\\nDao, T., Fu, D., Ermon, S., Rudra, A., and R´e, C. Flashat-\\ntention: Fast and memory-efficient exact attention with\\nio-awareness. Advances in Neural Information Process-\\ning Systems, 35:16344–16359, 2022.\\nDavari, M., Asadi, N., Mudur, S., Aljundi, R., and\\nBelilovsky, E. Probing representation forgetting in super-\\nvised and unsupervised continual learning. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition, pp. 16712–16721, 2022.\\nFarajtabar, M., Azizan, N., Mott, A., and Li, A. Or-\\nthogonal gradient descent for continual learning.\\nIn\\nInternational Conference on Artificial Intelligence and\\nStatistics, pp. 3762–3773. PMLR, 2020. URL https:\\n//arxiv.org/abs/1910.07104.\\nFini, E., da Costa, V. G. T., Alameda-Pineda, X., Ricci,\\nE., Alahari, K., and Mairal, J. Self-supervised models\\nare continual learners. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition,\\npp. 9621–9630, 2022.\\nFrench, R. M.\\nCatastrophic forgetting in con-\\nin Cognitive Sci-\\nnectionist networks.\\nences,\\nISSN 13646613.\\nURL\\ndoi:\\nhttps://www.sciencedirect.com/science/\\narticle/abs/pii/S1364661399012942.\\n3(4):128–135,\\n10.1016/S1364-6613(99)01294-2.\\nTrends\\n1999.\\nGao, L., Biderman, S., Black, S., Golding, L., Hoppe, T.,\\nFoster, C., Phang, J., He, H., Thite, A., Nabeshima, N.,\\net al. The pile: An 800gb dataset of diverse text for\\nlanguage modeling. arXiv preprint arXiv:2101.00027,\\n2020.\\nGao, L., Tow, J., Biderman, S., Black, S., DiPofi, A.,\\nFoster, C., Golding, L., Hsu, J., McDonell, K., Muen-\\nnighoff, N., Phang, J., Reynolds, L., Tang, E., Thite, A.,\\nWang, B., Wang, K., and Zou, A. A framework for few-\\nshot language model evaluation, September 2021. URL\\nhttps://doi.org/10.5281/zenodo.5371628.\\nGururangan, S., Marasovi´c, A., Swayamdipta, S., Lo, K.,\\nBeltagy, I., Downey, D., and Smith, N. A. Don’t stop\\npretraining: Adapt language models to domains and tasks.\\narXiv preprint arXiv:2004.10964, 2020. URL https:\\n//arxiv.org/abs/2004.10964.\\n7\\nContinual Pre-Training of Large Language Models: How to (re)warm-up your model?\\nGururangan, S., Lewis, M., Holtzman, A., Smith, N. A.,\\nand Zettlemoyer, L. Demix layers: Disentangling\\narXiv\\ndomains for modular language modeling.\\npreprint arXiv:2108.05036, 2021. URL https://\\narxiv.org/abs/2108.05036.\\nHan, R., Ren, X., and Peng, N. ECONET: Effective con-\\ntinual pretraining of language models for event tempo-\\nral reasoning. In Proceedings of the 2021 Conference\\non Empirical Methods in Natural Language Process-\\ning, pp. 5367–5380, Online and Punta Cana, Domini-\\ncan Republic, November 2021. Association for Com-\\nputational Linguistics. doi: 10.18653/v1/2021.emnlp-\\nmain.436. URL https://aclanthology.org/\\n2021.emnlp-main.436.\\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\\nCai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A.,\\nWelbl, J., Clark, A., et al. Training compute-optimal large\\nlanguage models. arXiv preprint arXiv:2203.15556, 2022.\\nURL https://arxiv.org/abs/2203.15556.\\nIgl, M., Farquhar, G., Luketina, J., Boehmer, W., and\\nWhiteson, S. The impact of non-stationarity on gen-\\narXiv\\neralisation in deep reinforcement\\npreprint arXiv:2006.05826, 2020. URL https://\\narxiv.org/abs/2006.05826.pdf.\\nlearning.\\nJang, J., Ye, S., Yang, S., Shin, J., Han, J., Kim,\\nTowards contin-\\nG., Choi, S. J., and Seo, M.\\narXiv\\nual knowledge learning of language models.\\npreprint arXiv:2110.03215, 2021. URL https://\\narxiv.org/abs/2110.03215.\\nKirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C.,\\nGustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo,\\nW.-Y., Doll´ar, P., and Girshick, R. Segment anything.\\narXiv:2304.02643, 2023.\\nKirkpatrick,\\nJ., Pascanu, R., Rabinowitz, N., Ve-\\nness, J., Desjardins, G., Rusu, A. A., Milan, K.,\\nQuan, J., Ramalho, T., Grabska-Barwinska, A., et al.\\nOvercoming catastrophic forgetting in neural net-\\nworks. Proc. of the national academy of sciences,\\n2017. URL https://www.pnas.org/content/\\npnas/114/13/3521.full.pdf.\\nLange, M. D., van de Ven, G. M., and Tuytelaars, T. Con-\\ntinual evaluation for lifelong learning: Identifying the\\nIn The Eleventh International Confer-\\nstability gap.\\nence on Learning Representations, 2023. URL https:\\n//openreview.net/forum?id=Zy350cRstc6.\\nLesort, T., Caselles-Dupr´e, H., Garcia-Ortiz, M., Goudou,\\nJ.-F., and Filliat, D. Generative models from the per-\\nIn IJCNN - Interna-\\nspective of continual learning.\\ntional Joint Conference on Neural Networks, Budapest,\\nHungary, Jul 2019. URL https://hal.archives-\\nouvertes.fr/hal-01951954.\\nLesort, T., Caccia, M., and Rish, I. Understanding contin-\\nual learning settings with data distribution drift analysis.\\narXiv preprint arXiv:2104.01678, 2021.\\nLesort, T., Ostapenko, O., Rodriguez, P., Arefin, M. R.,\\nMisra, D., Charlin, L., and Rish, I. Challenging common\\nassumptions about catastrophic forgetting. 2023.\\nJang, J., Ye, S., Lee, C., Yang, S., Shin, J., Han, J., Kim, G.,\\nand Seo, M. Temporalwiki: A lifelong benchmark for\\ntraining and evaluating ever-evolving language models.\\n2022.\\nLiang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D.,\\nYasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar,\\nA., et al. Holistic evaluation of language models. arXiv\\npreprint arXiv:2211.09110, 2022.\\nJin, X., Zhang, D., Zhu, H., Xiao, W., Li, S.-W., Wei,\\nX., Arnold, A., and Ren, X. Lifelong pretraining:\\nContinually adapting language models to emerging cor-\\nIn Proceedings of BigScience Episode #5 –\\npora.\\nWorkshop on Challenges & Perspectives in Creating\\nLarge Language Models, pp. 1–16, May 2022. doi:\\n10.18653/v1/2022.bigscience-1.1. URL https://\\naclanthology.org/2022.bigscience-1.1.\\nKe, Z., Shao, Y., Lin, H., Konishi, T., Kim, G., and Liu,\\nB. Continual pre-training of language models. In The\\nEleventh International Conference on Learning Represen-\\ntations, 2023a. URL https://openreview.net/\\nforum?id=m GDIItaI3o.\\nKe, Z., Shao, Y., Lin, H., Xu, H., Shu, L., and Liu, B.\\nAdapting a language model while preserving its general\\nknowledge. arXiv preprint arXiv:2301.08986, 2023b.\\nLoshchilov, I. and Hutter, F. Decoupled weight decay reg-\\nIn International Conference on Learning\\nularization.\\nRepresentations, 2018. URL https://arxiv.org/\\nabs/1711.05101.\\nLoureiro, D., Barbieri, F., Neves, L., Espinosa Anke, L., and\\nCamacho-collados, J. TimeLMs: Diachronic language\\nmodels from Twitter. In Proceedings of the 60th Annual\\nMeeting of the Association for Computational Linguistics:\\nSystem Demonstrations, pp. 251–260, Dublin, Ireland,\\nMay 2022. Association for Computational Linguistics.\\ndoi: 10.18653/v1/2022.acl-demo.25. URL https://\\naclanthology.org/2022.acl-demo.25.\\nMadaan, D., Yoon, J., Li, Y., Liu, Y., and Hwang, S. J.\\nRepresentational continuity for unsupervised continual\\nlearning. In International Conference on Learning Rep-\\nresentations, 2021.\\n8\\nContinual Pre-Training of Large Language Models: How to (re)warm-up your model?\\nMirzadeh, S.\\nI., Farajtabar, M., Pascanu, R., and\\nGhasemzadeh, H. Understanding the role of training\\nregimes in continual learning. Advances in Neural Infor-\\nmation Processing Systems, 33:7308–7320, 2020.\\nNguyen, C. V., Li, Y., Bui, T. D., and Turner, R. E.\\nVariational continual learning. In International Confer-\\nence on Learning Representations, 2018. URL https:\\n//arxiv.org/abs/1710.10628.\\nNVIDIA. NVIDIA Collective Communication Library\\n(NCCL). https://docs.nvidia.com/deeplearning/sdk/nccl-\\ndeveloper-guide/docs/index.html, 2016.\\nAccessed:\\nSeptember 8, 2023.\\nNVIDIA. Pytorch extension with NVIDIA-maintained utili-\\nties to streamline mixed precision and distributed training.\\nhttps://nvidia.github.io/apex/, 2019. Accessed: Septem-\\nber 8, 2023.\\nOquab, M., Darcet, T., Moutakanni, T., Vo, H. V.,\\nSzafraniec, M., Khalidov, V., Fernandez, P., Haziza, D.,\\nMassa, F., El-Nouby, A., Howes, R., Huang, P.-Y., Xu,\\nH., Sharma, V., Li, S.-W., Galuba, W., Rabbat, M., As-\\nsran, M., Ballas, N., Synnaeve, G., Misra, I., Jegou, H.,\\nMairal, J., Labatut, P., Joulin, A., and Bojanowski, P.\\nDinov2: Learning robust visual features without supervi-\\nsion, 2023.\\nOstapenko, O., Lesort, T., Rodr´ıguez, P., Arefin, M. R.,\\nDouillard, A., Rish, I., and Charlin, L. Continual learning\\nwith foundation models: An empirical study of latent\\nreplay, 2022.\\nPaszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E.,\\nDeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer,\\nA. Automatic Differentiation in PyTorch. 2017.\\nQin, Y., Zhang, J., Lin, Y., Liu, Z., Li, P., Sun, M., and Zhou,\\nJ. Elle: Efficient lifelong pre-training for emerging data.\\narXiv preprint arXiv:2203.06311, 2022. URL https:\\n//arxiv.org/abs/2203.06311.\\nRae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoff-\\nmann, J., Song, F., Aslanides, J., Henderson, S., Ring,\\nR., Young, S., et al. Scaling language models: Meth-\\nods, analysis & insights from training gopher. arXiv\\npreprint arXiv:2112.11446, 2021. URL https://\\narxiv.org/abs/2112.11446.\\nRao, D., Visin, F., Rusu, A. A., Teh, Y. W., Pascanu, R.,\\nand Hadsell, R. Continual unsupervised representation\\nlearning. 2019. URL https://arxiv.org/pdf/\\n1910.14481.pdf.\\nRasley, J., Rajbhandari, S., Ruwase, O., and He, Y. Deep-\\nspeed: System optimizations enable training deep learn-\\ning models with over 100 billion parameters. In Proceed-\\nings of the 26th ACM SIGKDD International Conference\\non Knowledge Discovery & Data Mining, pp. 3505–3506,\\n2020.\\nRebuffi, S.-A., Kolesnikov, A., Sperl, G., and Lampert, C. H.\\nicarl: Incremental classifier and representation learning.\\nIn Proceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition, pp. 2001–2010, 2017.\\nURL https://arxiv.org/abs/1611.07725.\\nScao, T. L., Fan, A., Akiki, C., Pavlick, E., Ili´c, S., Hesslow,\\nD., Castagn´e, R., Luccioni, A. S., Yvon, F., Gall´e, M.,\\net al. Bloom: A 176b-parameter open-access multilingual\\nlanguage model. arXiv preprint arXiv:2211.05100, 2022.\\nURL https://arxiv.org/abs/2211.05100.\\nScialom, T., Chakrabarty, T., and Muresan, S. Fine-tuned\\nlanguage models are continual learners. In Proceedings\\nof the 2022 Conference on Empirical Methods in Natural\\nLanguage Processing, pp. 6107–6122, 2022.\\nSeff, A., Beatson, A., Suo, D., and Liu, H. Contin-\\nual learning in generative adversarial nets. CoRR,\\nabs/1705.08395, 2017. URL http://arxiv.org/\\nabs/1705.08395.\\nSoboleva, D., Al-Khateeb, F., Myers, R., Steeves, J. R.,\\nHestness, J., and Dey, N.\\nSlimPajama: A 627B\\ntoken cleaned and deduplicated version of RedPa-\\nhttps://www.cerebras.net/blog/\\njama.\\nslimpajama-a-627b-token-cleaned-and-\\ndeduplicated-version-of-redpajama, 2023.\\nhttps://huggingface.co/datasets/\\nURL\\ncerebras/SlimPajama-627B.\\nTogether.xyz.\\nRedpajama: An open source recipe\\nURL\\nto reproduce llama training dataset, 2023.\\nhttps://github.com/togethercomputer/\\nRedPajama-Data.\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\\nAzhar, F., et al. Llama: Open and efficient foundation\\nlanguage models. arXiv preprint arXiv:2302.13971, 2023.\\nURL https://arxiv.org/abs/2302.13971.\\nWinata, G. I., Xie, L., Radhakrishnan, K., Wu, S., Jin, X.,\\nCheng, P., Kulkarni, M., and Preotiuc-Pietro, D. Over-\\ncoming catastrophic forgetting in massively multilingual\\ncontinual learning. arXiv preprint arXiv:2305.16252,\\n2023.\\nWolf, T., Debut, L., Sanh, V., Chaumond, J., De-\\nlangue, C., Moi, A., Cistac, P., Ma, C., Jernite, Y.,\\nPlu, J., Xu, C., Le Scao, T., Gugger, S., Drame,\\nTransformers:\\nM., Lhoest, Q., and Rush, A. M.\\npp.\\nState-of-the-Art Natural Language Processing.\\n9\\nContinual Pre-Training of Large Language Models: How to (re)warm-up your model?\\n38–45. Association for Computational Linguistics, Oc-\\nURL https://www.aclweb.org/\\ntober 2020.\\nanthology/2020.emnlp-demos.6.\\nXu, H., Liu, B., Shu, L., and Yu, P. S. Bert post-training\\nfor review reading comprehension and aspect-based sen-\\ntiment analysis. arXiv preprint arXiv:1904.02232, 2019.\\nYang, G., Hu, E. J., Babuschkin, I., Sidor, S., Farhi, D.,\\nPachocki, J., Liu, X., Chen, W., and Gao, J. Tensor\\nprograms v: Tuning large neural networks via zero-shot\\nIn NeurIPS 2021, March\\nhyperparameter transfer.\\nURL https://www.microsoft.com/\\n2022.\\nen-us/research/publication/tuning-\\nlarge-neural-networks-via-zero-shot-\\nhyperparameter-transfer/.\\nZhai, M., Chen, L., Tung, F., He, J., Nawhal, M., and Mori,\\nG. Lifelong gan: Continual learning for conditional im-\\nage generation. In Proceedings of the IEEE/CVF inter-\\nnational conference on computer vision, pp. 2759–2768,\\n2019.\\nZhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X.,\\nHou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z.,\\narXiv\\net al. A survey of large language models.\\npreprint arXiv:2303.18223, 2023. URL https://\\narxiv.org/abs/2303.18223.\\n10\\nContinual Pre-Training of Large Language Models: How to (re)warm-up your model?\\n', metadata={'heading': 'References\\n', 'content_font': 9, 'heading_font': 11}),\n",
       " Document(page_content='Figure 8. Pile validation loss of models trained from the fully converged checkpoint, the upstream saturation point, and 1/2 of the\\nupstream saturation point. The experiments for this figure are described in Sec. 4.5.\\nFigure 9. Training from a pre-trained checkpoint achieves lower Pile and SlimPajama validation loss faster than training from scratch.\\n', metadata={'heading': 'A. Upstream loss when fine-tuning various checkpoints.\\n', 'content_font': 8, 'heading_font': 11}),\n",
       " Document(page_content='', metadata={'heading': '11\\n', 'content_font': 0, 'heading_font': 9})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tomas\\Python\\Factored Cognition Primer\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ice.paper import Paragraph, Section, Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "SectionType = Literal[\"abstract\", \"main\", \"back\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snippet_to_paragraph_parser(snippets):\n",
    "    paragraphs = []\n",
    "    for snippet in snippets:\n",
    "        sections = [Section(title=snippet.metadata[\"heading\"])]\n",
    "        section_type = section_type_parser(snippet.metadata[\"heading\"])\n",
    "\n",
    "        data = {\n",
    "            \"sentences\": [snippet.page_content],\n",
    "            \"sections\": sections,\n",
    "            \"sectionType\": section_type,\n",
    "        }\n",
    "        par = Paragraph(**data)\n",
    "        paragraphs.append(par)\n",
    "    return Paper(paragraphs=paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def section_type_parser(section_str: str) -> SectionType:\n",
    "    if \"abstract\" in section_str.lower():\n",
    "        return \"abstract\"\n",
    "    if \"references\" in section_str.lower():\n",
    "        return \"back\"\n",
    "    else:\n",
    "        return \"main\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = snippet_to_paragraph_parser(semantic_snippets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kshitij Gupta * 1 2 Benjamin Th´erien * 1 2 Adam Ibrahim * 1 2 Mats L. Richter 1 2 Quentin Anthony 1 2 3\\nEugene Belilovsky 4 1 2 Irina Rish 1 2 Timoth´ee Lesort 1 2\\n'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper.paragraphs[0].sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv,dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(dotenv_path=\"../.ought-ice/.env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpaper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparagraphs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "paper.paragraphs[0].sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say this is a test\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    top_p=1,\n",
    "    n=1,\n",
    "    logprobs=True,\n",
    "    max_tokens=1,\n",
    "    top_logprobs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletionTokenLogprob(token='This', bytes=[84, 104, 105, 115], logprob=-0.00020735491, top_logprobs=[TopLogprob(token='This', bytes=[84, 104, 105, 115], logprob=-0.00020735491), TopLogprob(token='\"This', bytes=[34, 84, 104, 105, 115], logprob=-9.357522), TopLogprob(token='Sure', bytes=[83, 117, 114, 101], logprob=-9.853229), TopLogprob(token=' This', bytes=[32, 84, 104, 105, 115], logprob=-10.9564085), TopLogprob(token='this', bytes=[116, 104, 105, 115], logprob=-11.682382)])]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion.choices[0].logprobs.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes(tokens[0].bytes).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style='color: #FF00FF'>This</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 1\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "tokens = chat_completion.choices[0].logprobs.content\n",
    "\n",
    "colors = [\n",
    "    \"#FF00FF\",  # Magenta\n",
    "    \"#008000\",  # Green\n",
    "    \"#FF8C00\",  # Dark Orange\n",
    "    \"#FF0000\",  # Red\n",
    "    \"#0000FF\",  # Blue\n",
    "]\n",
    "\n",
    "color_idx = 0  # Initialize color index\n",
    "html_output = \"\"  # Initialize HTML output\n",
    "for t in tokens:\n",
    "    token_str = bytes(t.bytes).decode(\"utf-8\")  # Decode bytes to string\n",
    "\n",
    "    # Add colored token to HTML output\n",
    "    html_output += f\"<span style='color: {colors[color_idx]}'>{token_str}</span>\"\n",
    "\n",
    "    # Move to the next color\n",
    "    color_idx = (color_idx + 1) % len(colors)\n",
    "display(HTML(html_output))  # Display HTML output\n",
    "print(f\"Total number of tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, True, True, True, True, True, False, True, True]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[truncate_message(x.page_content) for x in semantic_snippets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ice.utils import make_gpt2_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = make_gpt2_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5661, 318, 257, 1332]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"this is a test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
